{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Obtaining MOSI through CMU-Multimodal Data SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Please refer to https://github.com/A2Zadeh/CMU-MultimodalDataSDK for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Installation\n",
    "In bash:\n",
    "\n",
    "git clone git@github.com:A2Zadeh/CMU-MultimodalDataSDK.git\n",
    "\n",
    "export PYTHONPATH=\"/path/to/cloned/directory/CMU-MultimodalDataSDK:$PYTHONPATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./CMU-MultimodalDataSDK/\")\n",
    "from mmdata import Dataloader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Merging and Accessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosi = Dataloader('http://sorena.multicomp.cs.cmu.edu/downloads/MOSI') # feed in the URL for the dataset. \n",
    "mosi_visual = mosi.facet()\n",
    "mosi_text = mosi.embeddings()\n",
    "mosi_audio = mosi.covarep()\n",
    "mosi_all = Dataset.merge(mosi_visual, mosi_text)\n",
    "mosi_all = Dataset.merge(mosi_all, mosi_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facet', 'covarep', 'embeddings']\n"
     ]
    }
   ],
   "source": [
    "print mosi_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Loading Train/Validation/Test Splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2iD-tVS8NPw\n"
     ]
    }
   ],
   "source": [
    "train_ids = mosi.train()\n",
    "#valid_ids = mosi.valid()\n",
    "#test_ids = mosi.test()\n",
    "vid = list(train_ids)[0]  \n",
    "print vid # print the first video id in training split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Access Segments and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = mosi_all['facet'][vid]['3'] # access the facet data in the first video for the 3rd segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many features in a segment. Note that number of features may be different from different modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "9\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "print len(mosi_all['facet'][vid]['3']) # number of visual features (30 features per second)\n",
    "print len(mosi_all['embeddings'][vid]['3']) # number of text features (1 feature per word)\n",
    "print len(mosi_all['covarep'][vid]['3']) # number of audio features (100 features per second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of each feature is \"(start_time_1, end_time_1, numpy.array([...]))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.027177550000001105, 0.06051085000000111, array([ 4.91000e+02,  1.87000e+02,  2.79000e+02,  2.79000e+02,\n",
      "       -2.45659e+00, -1.37122e+00, -1.71153e+00, -5.12618e-01,\n",
      "        5.34201e-01, -1.32983e+00, -1.14383e+00,  5.05123e-01,\n",
      "       -8.44108e-01, -5.20902e-01, -2.22139e+00, -1.70561e+00,\n",
      "        1.12229e-04,  8.84197e-03,  3.62235e-03,  8.39275e-02,\n",
      "        6.54749e-01,  3.75348e-02,  6.69964e-02,  3.62424e-01,\n",
      "        1.33997e-02,  5.98657e-02,  1.96531e-10,  4.67588e-07,\n",
      "        1.43613e+00,  9.61095e-01,  1.94466e-01, -5.75646e-01,\n",
      "       -8.92864e-01,  7.80307e-01, -2.38334e+00, -1.24573e-01,\n",
      "       -4.10652e-01, -9.14627e-01, -1.10465e+00, -9.49969e-01,\n",
      "       -3.46717e-01,  5.19613e-01, -2.13318e-01, -1.92194e+00,\n",
      "        2.29312e-01,  5.81445e-01], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print mosi_all['facet'][vid]['3'][0] # print the first visual feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Features Alignment between Modalities\n",
    "\n",
    "Perform alignment for different modality features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#aligned_text = mosi_all.align('embeddings') # aligning features according to the textual features.\n",
    "#aligned_audio = mosi_all.align('covarep') # aligning features according to the audio features.\n",
    "aligned_visual = mosi_all.align('facet') # aligning features according to the visual features.\n",
    "\n",
    "# assert the features is being aligned!\n",
    "print len(aligned_visual['embeddings'][vid]['3']) == len(aligned_visual['facet'][vid]['3']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  Loading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4\n"
     ]
    }
   ],
   "source": [
    "labels = mosi.sentiments()\n",
    "print labels[vid]['3'] # print the labels for the segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Tutorials\n",
    "\n",
    "Install Keras and at least one of the backend (Tensorflow or Theano). Play with `early_fusion_lstm.py` in the `CMU-MultimodalDataSDK/examples`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
