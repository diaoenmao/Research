{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Table of Contents\n",
    "1. [Tutorial](#tutorial)\n",
    "2. [Dataset](#dataset)  \n",
    "    2.1 [Visual](#visual)  \n",
    "    2.2 [Audio](#audio)  \n",
    "    2.3 [Text](#text)\n",
    "3. [Module](#module)  \n",
    "    3.1 [Linear Layer](#linear_layer)  \n",
    "    3.2 [Convolution Layer](#convolution_layer)  \n",
    "    3.3 [Pooling Layer](#pooling_layer)  \n",
    "4. [Model](#model)  \n",
    "    4.1 [Convolution](#convolution)  \n",
    "    4.2 [Recurrent](#recurrent)  \n",
    "5. [Loss](#loss)  \n",
    "    4.1 [Regression](#convolution)  \n",
    "    4.2 [Classification](#recurrent)  \n",
    "6. [Activation](#activation)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tutorial\"></a>\n",
    "## 1. Tutorial\n",
    "* [CS231n](http://cs231n.github.io/convolutional-networks/)\n",
    "* [Convolution Arithmetic](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "## 2. Dataset\n",
    "<a id=\"visual\"></a>\n",
    "### 2.1 Visual\n",
    "#### 2.1.1 MNIST\n",
    "The MNIST database of handwritten digits 0-9 has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "![MNIST](./img/MNIST.jpg)\n",
    "* URL:\n",
    "    * [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)\n",
    "        \n",
    "#### 2.1.2 COCO\n",
    "COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features:\n",
    "* Object segmentation\n",
    "* Recognition in context\n",
    "* Superpixel stuff segmentation\n",
    "* 330K images (>200K labeled)\n",
    "* 1.5 million object instances\n",
    "* 80 object categories\n",
    "* 91 stuff categories\n",
    "* 5 captions per image\n",
    "* 250,000 people with keypoints\n",
    "![COCO](./img/COCO.jpg)\n",
    "* URL:\n",
    "    * [COCO](http://cocodataset.org/#home)\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/torchvision/datasets.html#coco)\n",
    "    \n",
    "#### 2.1.3 WebVision \n",
    "Previously known as Imagenet. The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. Our goal is to disentangle the deep learning techniques from huge human labor on annotating large-scale vision dataset. We release this large scale web images dataset as a benchmark to advance the research on learning from web data, including weakly supervised visual representation learning, visual transfer learning, text and vision, etc. (see recommended settings for the WebVision dataset).\n",
    "\n",
    "Similar to WebVisioin 1.0 dataset, the WebVision 2.0 dataset also contains images crawled from the Flickr website and Google Images search. In this new version, we extend the number of visual concepts from 1,000 to 5,000, and the total number of trianing images reaches 16 million. The 5,000 visual concepts contains the original 1,000 concepts in WebVision 1.0 dataset, and additional 4,000 synsets in ImageNet with the most number of images. Semantically overlapped synsets are removed, such that there is no pair of synsets that one is the parent or child of the other. All 5,000 visual concepts have their corresponding synsets in the ImageNet dataset, so a bunch of existing approaches can be directly investigated and compared to the models trained from the human annotated ImageNet dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains around 250K images (up to 50 images per category) is provided to facilitate the algorithmic development.\n",
    "![WebVision](./img/WebVision.jpg)\n",
    "* URL:\n",
    "    * [Imagenet](http://www.image-net.org/)\n",
    "    * [WebVision](https://www.vision.ee.ethz.ch/webvision/)\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/torchvision/datasets.html#coco)\n",
    "\n",
    "#### 2.1.4 CIFAR\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "![CIFAR](./img/CIFAR.jpg)\n",
    "* URL:\n",
    "    * [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)\n",
    "\n",
    "#### 2.1.4 SVHN\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. \n",
    "![SVHN](./img/SVHN.jpg)\n",
    "* URL:\n",
    "    * [SVHN](http://ufldl.stanford.edu/housenumbers/)\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/torchvision/datasets.html#svhn)\n",
    "    \n",
    "<a id=\"audio\"></a>    \n",
    "### 2.2 Audio\n",
    "    \n",
    "<a id=\"text\"></a> \n",
    "### 2.3 Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"module\"></a>\n",
    "## 3. Module\n",
    "<a id=\"linear_layer\"></a>\n",
    "### 3.1 Linear Layer\n",
    "#### 3.1.1 Linear\n",
    "* Other names: Fully Connected, Dense\n",
    "* Parameters: $C_{in},C_{out}$\n",
    "* Formula:  \n",
    "\\begin{equation*}\n",
    "\\text{out}(N, C_{out})=\\text{input}(N, C_{in})(\\text{weight}(C_{out},C_{in}))^{T} + \\text{bias}(C_{out})\n",
    "\\end{equation*}\n",
    "* Shape:\n",
    "    * Input: $(N,∗,C_{in})$ where $∗$ means any number of additional dimensions\n",
    "    * Output: $(N,∗,C_{out})$ where all but the last dimension are the same shape as the input\n",
    "    * Weight:  $(C_{out}, C_{in})$\n",
    "    * Bias: $(C_{out})$\n",
    "* URL:\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "\n",
    "<a id=\"convolution_layer\"></a>\n",
    "### 3.2 Convolution Layer\n",
    "#### 3.2.1 Conv (2D)\n",
    "* Parameters: $C_{in},C_{out},\\text{kernel_size},N_{stride},N_{padding},N_{dilation},N_{groups}$\n",
    "* Formula:  \n",
    "\\begin{equation*}\n",
    "\\text{out}(N_i, C_{out_j}) = \\text{bias}(C_{out_j}) +\n",
    "                        \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{out_j}, k) \\star \\text{input}(N_i, k)\n",
    "\\end{equation*}\n",
    "where $⋆$ is the valid cross-correlation operator\n",
    "* Shape:\n",
    "    * Input: $(N, C_{in}, H_{in}, W_{in})$ \n",
    "    * Output: $(N, C_{out}, H_{out}, W_{out})$ where\n",
    "\\begin{align}\\begin{aligned}\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 * N_{padding}[0] - N_{dilation}[0] * (\\text{kernel_size}[0] - 1) - 1}{N_{stride}[0]} + 1\\right\\rfloor\\\\\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 * N_{padding}[1] - N_{dilation}[1] * (\\text{kernel_size}[1] - 1) - 1}{N_{stride}[1]} + 1\n",
    "\\right\\rfloor\n",
    "\\end{aligned}\\end{align}\n",
    "    * Weight:  $(C_{out}, C_{in}, \\text{kernel_size}[0], \\text{kernel_size}[1])$\n",
    "    * Bias: $(C_{out})$\n",
    "* URL:\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
    "    \n",
    "#### 3.2.1 ConvTranspose (2D)\n",
    "* Parameters: $C_{in},C_{out},\\text{kernel_size},N_{stride},N_{padding},N_{output_padding},N_{dilation},N_{groups}$\n",
    "* Formula:  \n",
    "\\begin{equation*}\n",
    "\\text{out}(N_i, C_{out_j}) = \\text{bias}(C_{out_j}) +\n",
    "                        \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{out_j}, k) \\star \\text{input}(N_i, k)\n",
    "\\end{equation*}\n",
    "where $⋆$ is the valid cross-correlation operator\n",
    "* Shape:\n",
    "    * Input: $(N, C_{in}, H_{in}, W_{in})$ \n",
    "    * Output: $(N, C_{out}, H_{out}, W_{out})$ where\n",
    "\\begin{align}\\begin{aligned}\n",
    "H_{out} = (H_{in} - 1) * N_{stride}[0] - 2 * N_{padding}[0]+ \\text{kernel_size}[0] + N_{output\\_padding}[0]\\\\\n",
    "W_{out} = (W_{in} - 1) * N_{stride}[1] - 2 * N_{padding}[1]+ \\text{kernel_size}[1] + N_{output\\_padding}[1]\n",
    "\\end{aligned}\\end{align}\n",
    "    * Weight:  $(C_{out}, C_{in}, \\text{kernel_size}[0], \\text{kernel_size}[1])$\n",
    "    * Bias: $(C_{out})$\n",
    "* URL:\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
    "\n",
    "<a id=\"pooling_layer\"></a>\n",
    "### 3.3 Pooling Layer\n",
    "#### 3.3.1 MaxPool (2D)\n",
    "* Parameters: $C_{in},C_{out},\\text{kernel_size},N_{stride},N_{padding},N_{dilation}$\n",
    "* Formula:  \n",
    "\\begin{equation*}\n",
    "\\text{out}(N_i, C_j, h, w)  = \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1}\n",
    "                       \\text{input}(N_i, C_j, \\text{stride}[0] * h + m, \\text{stride}[1] * w + n)\n",
    "\\end{equation*}\n",
    "* Shape (2D):\n",
    "    * Input: $(N, C_{in}, H_{in}, W_{in})$ \n",
    "    * Output: $(N, C_{out}, H_{out}, W_{out})$ where\n",
    "\\begin{align}\\begin{aligned}\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 * N_{padding}[0] - N_{dilation}[0] * (\\text{kernel_size}[0] - 1) - 1}{N_{stride}[0]} + 1\\right\\rfloor\\\\\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 * N_{padding}[1] - N_{dilation}[1] * (\\text{kernel_size}[1] - 1) - 1}{N_{stride}[1]} + 1\n",
    "\\right\\rfloor\n",
    "\\end{aligned}\\end{align}\n",
    "    * Kernel Size:  $(kH,kW)$\n",
    "* URL:\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#maxpool2d)\n",
    "    \n",
    "#### 3.3.2 AvgPool (2D)\n",
    "* Parameters: $C_{in},C_{out},\\text{kernel_size},N_{stride},N_{padding}$\n",
    "* Formula:  \n",
    "\\begin{equation*}\n",
    "\\text{out}(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}\n",
    "                       \\text{input}(N_i, C_j, \\text{stride}[0] * h + m, \\text{stride}[1] * w + n)\n",
    "\\end{equation*}\n",
    "* Shape (2D):\n",
    "    * Input: $(N, C_{in}, H_{in}, W_{in})$ \n",
    "    * Output: $(N, C_{out}, H_{out}, W_{out})$ where\n",
    "\\begin{align}\\begin{aligned}\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 * N_{padding}[0] - \\text{kernel_size}[0]}{N_{stride}[0]} + 1\\right\\rfloor\\\\\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 * N_{padding}[1] - \\text{kernel_size}[1]}{N_{stride}[1]} + 1\\right\\rfloor\n",
    "\\end{aligned}\\end{align}\n",
    "    * Kernel Size:  $(kH,kW)$\n",
    "* URl:\n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#avgpool2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"convolution\"></a>\n",
    "### 4.1 Convolution\n",
    "#### 4.1.1 LeNet5\n",
    "LeNet5 is a pioneering 7-level convolutional network by LeCun et al in 1998, that classifies digits, was applied by several banks to recognise hand-written numbers on checks (cheques) digitized in 32x32 pixel greyscale inputimages. The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.\n",
    "![LeNet5](./img/LeNet5.jpg)\n",
    "* URl:\n",
    "    * [LeNet5](http://yann.lecun.com/exdb/lenet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "   def __init__(self):\n",
    "       super().__init__()\n",
    "       self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "       self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "       self.fc1 = nn.Linear(16*5*5, 120)\n",
    "       self.fc2 = nn.Linear(120, 84)\n",
    "       self.fc3 = nn.Linear(84, 10)\n",
    "   def forward(self, x):\n",
    "       x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "       x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "       x = x.view(x.size(0), -1)\n",
    "       x = F.relu(self.fc1(x))\n",
    "       x = F.relu(self.fc2(x))\n",
    "       x = self.fc3(x)\n",
    "       return x\n",
    "    \n",
    "# Current Implementation     \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 AlexNet\n",
    "In 2012, AlexNet significantly outperformed all the prior competitors and won the challenge by reducing the top-5 error from 26% to 15.3%. The second place top-5 error rate, which was not a CNN variation, was around 26.2%.\n",
    "The network had a very similar architecture as LeNet by Yann LeCun et al but was deeper, with more filters per layer, and with stacked convolutional layers. It consisted 11x11, 5x5,3x3, convolutions, max pooling, dropout, data augmentation, ReLU activations, SGD with momentum. It attached ReLU activations after every convolutional and fully-connected layer. AlexNet was trained for 6 days simultaneously on two Nvidia Geforce GTX 580 GPUs which is the reason for why their network is split into two pipelines. AlexNet was designed by the SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever.\n",
    "![AlexNet](./img/AlexNet.jpg)\n",
    "* URl:\n",
    "    * [AlexNet](https://en.wikipedia.org/wiki/AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 VGGNet\n",
    "The runner-up at the ILSVRC 2014 competition is dubbed VGGNet by the community and was developed by Simonyan and Zisserman . VGGNet consists of 16 convolutional layers and is very appealing because of its very uniform architecture. Similar to AlexNet, only 3x3 convolutions, but lots of filters. Trained on 4 GPUs for 2–3 weeks. It is currently the most preferred choice in the community for extracting features from images. The weight configuration of the VGGNet is publicly available and has been used in many other applications and challenges as a baseline feature extractor. However, VGGNet consists of 138 million parameters, which can be a bit challenging to handle.\n",
    "![VGGNet_1](./img/VGGNet_1.jpg)\n",
    "![VGGNet_2](./img/VGGNet_2.jpg)\n",
    "* URl:\n",
    "    * [VGGNet](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 InceptionNet\n",
    "#### 4.1.4.1 InceptionNet v1\n",
    "The winner of the ILSVRC 2014 competition was GoogleNet(a.k.a. Inception V1) from Google. It achieved a top-5 error rate of 6.67%! This was very close to human level performance which the organisers of the challenge were now forced to evaluate. As it turns out, this was actually rather hard to do and required some human training in order to beat GoogLeNets accuracy. After a few days of training, the human expert (Andrej Karpathy) was able to achieve a top-5 error rate of 5.1%(single model) and 3.6%(ensemble). The network used a CNN inspired by LeNet but implemented a novel element which is dubbed an inception module. It used batch normalization, image distortions and RMSprop. This module is based on several very small convolutions in order to drastically reduce the number of parameters. Their architecture consisted of a 22 layer deep CNN but reduced the number of parameters from 60 million (AlexNet) to 4 million. We have different size of convolution kernels and we hope to have features from different scale.\n",
    "![InceptionNet_1](./img/InceptionNet_1.jpg)\n",
    "![InceptionNet_2](./img/InceptionNet_2.jpg)\n",
    "* URl:\n",
    "    * [InceptionNet_v1](http://arxiv.org/abs/1409.4842)\n",
    "\n",
    "#### 4.1.4.2 InceptionNet v2, v3\n",
    "* Add [Batch Normalization Layer](#bn_layer)\n",
    "* The 5x5 convolutional layers are replaced by two consecutive layer of 3x3 convolutions with up to 128 filters to reduce the number of parameters needed.\n",
    "* Factorize 2d Convolution into two 1d Convolution. 7x7 convolution is factorized into two（1x7,7x1）convolution，3x3 as（1x3,3x1). Here approximate 2d Convolution with less parameters and then we can use the extra computation power to go deeper.\n",
    "![InceptionNet_3](./img/InceptionNet_3.jpg)\n",
    "* URl:\n",
    "    * [InceptionNet_v2_3](http://arxiv.org/abs/1512.00567)\n",
    "    \n",
    "#### 4.1.4.3 InceptionNet v4\n",
    "Utilize Residual Network to enchance the performance of InceptionNet\n",
    "![InceptionNet_3](./img/InceptionNet_4.jpg)\n",
    "* URl:\n",
    "    * [InceptionNet_v4](https://arxiv.org/abs/1602.07261)\n",
    "    * [Pytorch](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 ResNet\n",
    "At the ILSVRC 2015, the so-called Residual Neural Network (ResNet) by Kaiming He et al introduced anovel architecture with “skip connections” and features heavy batch normalization. Such skip connections are also known as gated units or gated recurrent units and have a strong similarity to recent successful elements applied in RNNs. Thanks to this technique they were able to train a NN with 152 layers while still having lower complexity than VGGNet. It achieves a top-5 error rate of 3.57% which beats human-level performance on this dataset.\n",
    "ResNet solves the gradient vanishing issue and it become possible to go far deeper to 100 even 1000 layers.\n",
    "![ResNet_1](./img/ResNet_1.jpg)\n",
    "![ResNet_2](./img/ResNet_2.jpg)\n",
    "* URl:\n",
    "    * [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "    * [Pytorch](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)\n",
    "    * [Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 WideResNet\n",
    "Because ResNet is so deep，many residual block only provide limited information.16 layer WideResNet can achieve 1000 layer ResNet result. The largest contribution of ResNet is Residual block, going deep is not necessary. The parameters of WideResNet and basic ResNet are similar but owing to GPU parallel computation. WideResNet is about 8 times faster than ResNet.\n",
    "* Width or number of channels enchance the performance\n",
    "* Increasing depth and width is valid until pararmeter space is too large to regularize\n",
    "* For the same amount of parameters, increasing width is better than depth\n",
    "![WideResNet](./img/WideResNet.jpg)\n",
    "* URl:\n",
    "    * [WideResNet](https://arxiv.org/abs/1605.07146)\n",
    "    * [Pytorch](https://github.com/szagoruyko/wide-residual-networks/blob/master/pytorch/resnet.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.6 XceptionNet\n",
    "The hypothesis: \"cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly.\" Instead of partitioning input data into several compressed chunks, it maps the spatial correlations for each output channel separately, and then performs a 1x1 depthwise convolution to capture cross-channel correlation.\n",
    "![XceptionNet](./img/XceptionNet.jpg)\n",
    "* URl:\n",
    "    * [XceptionNet](https://arxiv.org/abs/1610.02357)\n",
    "    * [Pytorch](https://github.com/tstandley/Xception-PyTorch/blob/master/xception.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.7 ShuffleNet\n",
    "ShuffleNet is an efficient convolutional neural network architecture for mobile devices. Grouped convolution is a variant of convolution where the channels of the input feature map are grouped and convolution is performed independently for each grouped channels.Channel shuffle is an operation (layer) which changes the order of the channels. This operation is implemented by tensor reshape and transpose. The main contribution of this kind of Net is to reduce the number of parameters and computation while still maintain its performance. These kind of models can be used in resource scarce scenario like in moblie application.\n",
    "\n",
    "![ShuffleNet](./img/ShuffleNet.jpg)\n",
    "* URl:\n",
    "    * [ShuffleNet](https://arxiv.org/abs/1707.01083)\n",
    "    * [Group](https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d)\n",
    "    * [Pytorch](https://github.com/kuangliu/pytorch-cifar/blob/master/models/shufflenet.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.8 SENet\n",
    "SENet introduces a building block for CNNs that improves channel interdependencies at almost no computational cost. They were used at this years ImageNet competition and helped to improve the result from last year by 25%. Besides this huge performance boost, they can be easily added to existing architectures. The main idea is this:\n",
    "Let’s add parameters to each channel of a convolutional block so that the network can adaptively adjust the weighting of each feature map.\n",
    "The function is given an input convolutional block and the current number of channels it has. We squeeze each channel to a single numeric value using average pooling. A fully connected layer followed by a ReLU function adds the necessary nonlinearity. It’s output channel complexity is also reduced by a certain ratio. A second fully connected layer followed by a Sigmoid activation gives each channel a smooth gating function. At last, we weight each feature map of the convolutional block based on the result of our side network.\n",
    "![SENet_1](./img/SENet_1.jpg)\n",
    "![SENet_2](./img/SENet_2.jpg)\n",
    "* URl:\n",
    "    * [SENet](https://arxiv.org/abs/1709.01507)\n",
    "    * [Pytorch](https://github.com/moskomule/senet.pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"recurrent\"></a>\n",
    "### 4.2 Recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loss\"></a>\n",
    "### 5. Loss\n",
    "#### 5.1 Regression\n",
    "#### 5.1.1 MAELoss\n",
    "* other names: L1Loss\n",
    "\\begin{equation*}\n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = \\left| x_n - y_n \\right|,\n",
    "\\end{equation*}\n",
    "* URL: \n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#torch.nn.L1Loss)\n",
    "    \n",
    "#### 5.1.2 MSELoss\n",
    "* other names: L2Loss\n",
    "\\begin{equation*}\n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = \\left( x_n - y_n \\right)^2,\n",
    "\\end{equation*}\n",
    "* URL: \n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)\n",
    "    \n",
    "#### 5.2 Classification\n",
    "#### 5.2.1 CrossEntropyLoss\n",
    "This loss combines Softmax and NLLLoss in one single loss\n",
    "\\begin{equation*}\n",
    "\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)\n",
    "\\end{equation*}\n",
    "* URL: \n",
    "    * [Pytorch](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"activation\"></a>\n",
    "### 6. Activation\n",
    "#### RELU\n",
    "\\begin{equation*}\n",
    "\\text{ReLU}(x)= \\max(0, x)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"organic\"></a>\n",
    "### 7. Organic Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oraganic Machine is a coherent artificial intelligence system which dynamically adjusts parameters and achitecture to adapt various signal and problem scenarios and ultimately mimics or surpasses human intelligence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"signal\"></a>\n",
    "### 7.1 Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"objective\"></a>\n",
    "### 7.2 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"achitecture\"></a>\n",
    "### 7.3 Achitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Exploring Neural Network\n",
    "Neural Network usually has a lot of parameters. There exists a trade-off among parameters, computation, and performance. The rule-of-thumb of exploring Neural Network Achitecture is to reduce the number of parameters and computation cost while achieving high performance<br> <br>\n",
    "\n",
    "Classical feature selection claims to find the features that are the most correlated with target while are the least correlated with themselves. We often need to come up with a distance metric to evaluate these two \"correlation\".  Besides, we also need to incorporate the metric into a search algorithm since the number of possible discrete state of $K$ features is $2^K$. Even if the overhead of computing distance metric is small, it become hard to explore Neural Network Achitecure due to computation limitation. <br> <br>\n",
    "\n",
    "Owing to server computation scarce, interval estimate by Monte Carlo sampling is also not attractive. Variational Inference is marginally acceptable if the number of parameters to sample is small, since even a single forward-pass can create nonnegligible overhead. The compromise between Bayesian and Frequentist is more of a compromise of computation here. So far, Bayesian Deep Learning framework is gradually getting attention for the sake of inference and interpretation. But we are still lack of a framework that can efficiently switching between point and interval estimate. Therefore, classical Bayesian Variable Selection technique are not suitable here because of computation. People are trying to use relaxation of discrete distribution called concrete distribution to directly compute the gradient. They apply this trick to Dropout and the so-called concrete Dropout adpatively adjusts Dropout rate in a Variational Inference framework. However, since Dropout is more of data-augmentation, its usage in variable selection has not been addressed. <br> <br>\n",
    "\n",
    "Rationally, the most widely used techinque in high-dimension data analysis should be appropriate in this setting. The so-called LASSO and its variants have been attractive to the researchers since the beginning. It can be naturally applied in a Gradient Descent algorithm and the parameters can be sparsified. It not only takes performance into account but also computation. However, it introduces another hyperparameter that is hard to tune. Using search algorithm like LARS is also not suitable owing to computation cost. Besides, we still need to compute the gradient of each paremeter in order to select features. Although it addresses the computation load in a farily high-dimensional problem, it fails in the situation allowing the paramter space to expand. Another gradient based method uses a sigmoid gate to weight features while the weight is determined by trainable parameters. Although this method performs well in LSTM and SENet, it is not scalable and efficient to introduce more parameters to solve a feature selection problem.<br> <br>\n",
    "\n",
    "It seems like computation is a more heavy load to balance. The techinque we need has to roughly satisfy following criterions:\n",
    "1. select features that are most correlated with target\n",
    "2. select features that are least correlated with themselves or perform certain decorrelation transformation\n",
    "3. selection process prefers not involving any searching, sampling, or gradient based algorithms which do not scale well with the number of parameters\n",
    "4. After applying selection and Gradient Descent, we also need to explore new parameter space by intializing or perturbing from existing parameters\n",
    "\n",
    "Pinciple Component Analysis is a simple dimension reduction tool. Its heaviest computation only involves an eigenvalue decomposition. With batch data and a finite number of features, PCA is possibly the cheapest way to select features. We can also decorrelate the features with whitening. Therefore, we greatly alleviate the computation load and satisfy the 2nd and 3rd criterion. To address the 1st criterion, it is possible to first screen features whose correlation with target is less than a threshold. At each layer, the number of input features is determined by the previous layer representation, and the number of output features is determined by computation power. In this way, all of the number of features of Neural Network can be determined dynamically with respect to a trade-off between performance and computaion. The details are listed following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter**: $\\epsilon$, pca cutoff ratio $c_{pca}$, exploration cutoff ratio $c_{p}$, running average momentum $m$  \n",
    "**For** iteration $e$   \n",
    "**Input**: Exploitation paremeter set $P_{e},|P_{e}|=p$, Exploration paremeter set $Q_{e},|Q_{e}|=q$, batch signal $X_{b\\times (p+q)}^e$, expected mean $\\bar{\\mu}$, expected projection matrix $\\bar{\\Sigma}_{p\\times p}^{-1/2}$  \n",
    "**Output**: batch signal after selection $\\tilde{X}_{b\\times k}^{e}$  \n",
    "**Forward Pass**  \n",
    "**1.** $X_{bxp}^{e},P_{e},Q_{e} = $ pre-screen$(X^{e},P_{e}+Q_{e},c_{p})$  \n",
    "**2.** $\\tilde{\\mu}_{1\\dots p} = \\frac{1}{b}\\sum\\limits_{i=1}^{b}X_{i,1\\dots p}^{e}$  \n",
    "**3.** $\\tilde{\\Sigma} = \\frac{1}{b}\\sum\\limits_{i=1}^{b}(X_{i}^{e}-\\tilde{\\mu})^{T}(X_{i}^{e}-\\tilde{\\mu})+\\epsilon I$  \n",
    "**4.** $\\tilde{\\Sigma} = USV^T$  \n",
    "**5.** $\\lambda_{1 \\dots p} = diag(S)^2$  \n",
    "**6.** select $k$ principle components $s.t. \\frac{\\sum\\limits_{i=1}^{k}\\lambda_{i}}{\\sum\\limits_{i=1}^{p}\\lambda_{i}}<c_{pca}$   \n",
    "**7.** $\\tilde{X} = U_{1 \\dots k}S_{1 \\dots k}(V^TX^{e})_{1 \\dots k} $  \n",
    "**8.** $\\bar{\\mu} = (1-m)\\bar{\\mu} + m\\tilde{\\mu}$  \n",
    "**9.** $\\bar{\\Sigma} = (1-m)\\bar{\\Sigma} + m\\tilde{\\Sigma}$  \n",
    "**Backward Pass**  \n",
    "**10.** Update $P$ with Back-Propagation $\\nabla L_P$  \n",
    "**11.** Update $Q$ by reinitialization, sampling from distribution $p(\\nabla L_P)$, or pertubing with random noise (cheaper than computing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
